"""
Complete Analysis: Signal-Adaptive Conformal Inference

This script runs all experiments with the corrected methodology:
1. Honest volatility signal (not "crowding")
2. Fixed algorithm (locally-weighted quantiles)
3. All factors including Mkt-RF
4. Statistical rigor (SEs, p-values)
5. Proper baselines (GARCH, bootstrap, naive scaling)
6. Subperiod analysis with honest interpretation

Author: Chorok Lee (KAIST)
Date: December 2024
"""

import numpy as np
import pandas as pd
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from signal_adaptive_conformal import (
    StandardConformalPredictor,
    SignalAdaptiveCI,
    VanillaVolatilityScaling,
    evaluate_all_methods,
    compute_conditional_coverage_with_stats
)
from volatility_signal import (
    compute_volatility_signal,
    compute_absolute_return_signal,
    analyze_signal_correlation,
    classify_volatility_regime,
    test_coverage_difference
)

# Try to import GARCH for baseline
try:
    from arch import arch_model
    HAS_GARCH = True
except ImportError:
    HAS_GARCH = False
    print("Note: arch package not installed. GARCH baseline will be skipped.")


def load_factor_data() -> pd.DataFrame:
    """Load Fama-French factor data."""
    data_path = Path(__file__).parent.parent / 'data' / 'ff_factors.csv'

    if data_path.exists():
        factors = pd.read_csv(data_path, index_col=0, parse_dates=True)
    else:
        # Download if not exists
        sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))
        from download_data import load_or_download_factors
        factors = load_or_download_factors()

    return factors


def run_main_coverage_analysis(factors: pd.DataFrame,
                                factor_names: list,
                                alpha: float = 0.1,
                                cal_fraction: float = 0.5) -> pd.DataFrame:
    """
    Main coverage analysis: Standard CP vs SA-CI vs Vol Scaling.

    Now includes all factors (including Mkt-RF) and proper statistics.
    """
    print("\n" + "="*80)
    print("MAIN COVERAGE ANALYSIS")
    print("="*80)

    all_results = []

    for factor in factor_names:
        if factor not in factors.columns:
            print(f"  Warning: {factor} not found, skipping")
            continue

        print(f"\nProcessing {factor}...")

        returns = factors[factor].dropna()
        signal = compute_volatility_signal(returns, window=12)

        # Align and drop NaN
        valid = signal.notna()
        returns_clean = returns[valid].values
        signal_clean = signal[valid].values

        # Use mean predictor
        n = len(returns_clean)
        cal_end = int(n * cal_fraction)
        pred = np.full(n, np.mean(returns_clean[:cal_end]))

        # Run evaluation
        results = evaluate_all_methods(
            returns_clean, pred, signal_clean,
            alpha=alpha, cal_fraction=cal_fraction, calibrate=True
        )

        # Compute p-values for improvement
        for method in ['saci', 'vol_scaling']:
            z, p = test_coverage_difference(
                results[method].coverage_high_vol,
                results[method].n_high_vol,
                results['standard'].coverage_high_vol,
                results['standard'].n_high_vol
            )

            all_results.append({
                'factor': factor,
                'method': method,
                'coverage_overall': results[method].coverage_overall,
                'coverage_high_vol': results[method].coverage_high_vol,
                'coverage_low_vol': results[method].coverage_low_vol,
                'se_overall': results[method].se_overall,
                'se_high_vol': results[method].se_high_vol,
                'se_low_vol': results[method].se_low_vol,
                'avg_width': np.mean(results[method].width),
                'width_high': np.mean(results[method].width[signal_clean[cal_end:] > np.median(signal_clean[cal_end:])]),
                'width_low': np.mean(results[method].width[signal_clean[cal_end:] <= np.median(signal_clean[cal_end:])]),
                'improvement_high': results[method].coverage_high_vol - results['standard'].coverage_high_vol,
                'z_stat': z,
                'p_value': p,
                'n_high': results[method].n_high_vol,
                'n_low': results[method].n_low_vol
            })

        # Add standard CP row
        all_results.append({
            'factor': factor,
            'method': 'standard',
            'coverage_overall': results['standard'].coverage_overall,
            'coverage_high_vol': results['standard'].coverage_high_vol,
            'coverage_low_vol': results['standard'].coverage_low_vol,
            'se_overall': results['standard'].se_overall,
            'se_high_vol': results['standard'].se_high_vol,
            'se_low_vol': results['standard'].se_low_vol,
            'avg_width': np.mean(results['standard'].width),
            'width_high': np.mean(results['standard'].width),
            'width_low': np.mean(results['standard'].width),
            'improvement_high': 0.0,
            'z_stat': np.nan,
            'p_value': np.nan,
            'n_high': results['standard'].n_high_vol,
            'n_low': results['standard'].n_low_vol
        })

    df = pd.DataFrame(all_results)
    return df


def run_signal_validity_analysis(factors: pd.DataFrame,
                                  factor_names: list) -> pd.DataFrame:
    """
    Analyze what our volatility signal actually measures.

    This provides transparency about signal properties.
    """
    print("\n" + "="*80)
    print("SIGNAL VALIDITY ANALYSIS")
    print("="*80)

    results = []

    for factor in factor_names:
        if factor not in factors.columns:
            continue

        returns = factors[factor].dropna()
        corr_stats = analyze_signal_correlation(returns, window=12)

        # Also compute correlation between signals
        vol_signal = compute_volatility_signal(returns)
        abs_ret_signal = compute_absolute_return_signal(returns)

        valid = vol_signal.notna() & abs_ret_signal.notna()
        signal_corr = np.corrcoef(vol_signal[valid], abs_ret_signal[valid])[0, 1]

        results.append({
            'factor': factor,
            'vol_vs_forward_vol': corr_stats['vol_signal_vs_forward_vol'],
            'abs_ret_vs_forward_vol': corr_stats['abs_ret_vs_forward_vol'],
            'vol_vs_abs_ret': signal_corr,
            'n_obs': corr_stats['n_obs']
        })

        print(f"\n{factor}:")
        print(f"  Vol signal vs forward vol: {corr_stats['vol_signal_vs_forward_vol']:.3f}")
        print(f"  Abs ret vs forward vol: {corr_stats['abs_ret_vs_forward_vol']:.3f}")
        print(f"  Vol vs abs ret correlation: {signal_corr:.3f}")

    return pd.DataFrame(results)


def run_subperiod_analysis(factors: pd.DataFrame,
                            factor_names: list,
                            periods: list = None) -> pd.DataFrame:
    """
    Subperiod analysis with honest interpretation.

    Key insight: Standard CP works fine within periods, but fails
    when calibrating on one regime and testing on another.
    """
    print("\n" + "="*80)
    print("SUBPERIOD ANALYSIS (Regime Change Robustness)")
    print("="*80)

    if periods is None:
        # Default: split at 1993
        periods = [
            ('1963-1993', '1963-01-01', '1993-12-31'),
            ('1994-2025', '1994-01-01', '2025-12-31'),
            ('Full Sample', None, None)
        ]

    results = []

    for factor in factor_names:
        if factor not in factors.columns:
            continue

        for period_name, start, end in periods:
            if start is None:
                subset = factors
            else:
                subset = factors.loc[start:end]

            if len(subset) < 100:
                continue

            returns = subset[factor].dropna()
            signal = compute_volatility_signal(returns, window=12)

            valid = signal.notna()
            returns_clean = returns[valid].values
            signal_clean = signal[valid].values

            if len(returns_clean) < 50:
                continue

            n = len(returns_clean)
            cal_end = int(n * 0.5)
            pred = np.full(n, np.mean(returns_clean[:cal_end]))

            eval_results = evaluate_all_methods(
                returns_clean, pred, signal_clean,
                alpha=0.1, cal_fraction=0.5, calibrate=True
            )

            for method in ['standard', 'saci']:
                results.append({
                    'factor': factor,
                    'period': period_name,
                    'method': method,
                    'coverage_overall': eval_results[method].coverage_overall,
                    'coverage_high_vol': eval_results[method].coverage_high_vol,
                    'coverage_low_vol': eval_results[method].coverage_low_vol,
                    'n_obs': len(returns_clean)
                })

    df = pd.DataFrame(results)

    # Print interpretation
    print("\nKey Finding: Regime Change Sensitivity")
    print("-" * 60)

    for factor in factor_names:
        factor_df = df[(df['factor'] == factor) & (df['method'] == 'standard')]
        if len(factor_df) >= 2:
            full = factor_df[factor_df['period'] == 'Full Sample']['coverage_high_vol'].values
            sub1 = factor_df[factor_df['period'] == '1963-1993']['coverage_high_vol'].values
            sub2 = factor_df[factor_df['period'] == '1994-2025']['coverage_high_vol'].values

            if len(full) > 0 and len(sub1) > 0 and len(sub2) > 0:
                print(f"\n{factor}:")
                print(f"  Within 1963-1993: {sub1[0]:.1%}")
                print(f"  Within 1994-2025: {sub2[0]:.1%}")
                print(f"  Full sample (cross-regime): {full[0]:.1%}")
                if full[0] < min(sub1[0], sub2[0]) - 0.05:
                    print(f"  â†’ Under-coverage appears only when calibration/test span regimes!")

    return df


def run_monte_carlo_validation(n_sim: int = 200,
                                n_obs: int = 500,
                                delta_values: list = None) -> pd.DataFrame:
    """
    Monte Carlo validation with known DGP.

    Simulates data with signal-dependent volatility.
    """
    print("\n" + "="*80)
    print("MONTE CARLO VALIDATION")
    print("="*80)

    if delta_values is None:
        delta_values = [0.0, 0.25, 0.5, 0.75, 1.0]

    np.random.seed(42)
    results = []

    for delta in delta_values:
        print(f"\nDelta = {delta}...")

        for sim in range(n_sim):
            # Generate signal (AR(1))
            signal = np.zeros(n_obs)
            rho = 0.7
            for t in range(1, n_obs):
                signal[t] = rho * signal[t-1] + np.sqrt(1 - rho**2) * np.random.randn()

            # Signal-dependent volatility
            sigma_base = 0.05
            volatility = sigma_base * (1 + delta * np.maximum(0, signal))

            # Generate returns
            y_true = np.random.randn(n_obs) * volatility
            y_pred = np.zeros(n_obs)

            # Evaluate
            eval_results = evaluate_all_methods(
                y_true, y_pred, np.abs(signal),
                alpha=0.1, cal_fraction=0.5, calibrate=False
            )

            for method in ['standard', 'saci', 'vol_scaling']:
                results.append({
                    'delta': delta,
                    'sim': sim,
                    'method': method,
                    'coverage_overall': eval_results[method].coverage_overall,
                    'coverage_high_vol': eval_results[method].coverage_high_vol,
                    'coverage_low_vol': eval_results[method].coverage_low_vol
                })

    df = pd.DataFrame(results)

    # Summary
    print("\nMonte Carlo Summary:")
    print("-" * 60)
    summary = df.groupby(['delta', 'method']).agg({
        'coverage_overall': ['mean', 'std'],
        'coverage_high_vol': ['mean', 'std'],
        'coverage_low_vol': ['mean', 'std']
    }).round(3)
    print(summary)

    return df


def run_garch_comparison(factors: pd.DataFrame,
                          factor_names: list) -> pd.DataFrame:
    """
    Compare against GARCH(1,1) prediction intervals.
    """
    if not HAS_GARCH:
        print("\nSkipping GARCH comparison (arch package not installed)")
        return pd.DataFrame()

    print("\n" + "="*80)
    print("GARCH BASELINE COMPARISON")
    print("="*80)

    results = []

    for factor in factor_names:
        if factor not in factors.columns:
            continue

        print(f"\nProcessing {factor}...")

        returns = factors[factor].dropna()
        signal = compute_volatility_signal(returns, window=12)

        valid = signal.notna()
        returns_clean = returns[valid]
        signal_clean = signal[valid]

        n = len(returns_clean)
        cal_end = int(n * 0.5)

        # Fit GARCH on calibration
        try:
            model = arch_model(returns_clean.iloc[:cal_end] * 100,
                             vol='GARCH', p=1, q=1, dist='t')
            fit = model.fit(disp='off')

            # Forecast for test period
            forecasts = fit.forecast(horizon=1, start=cal_end, reindex=False)
            cond_vol = np.sqrt(forecasts.variance.values.flatten()) / 100

            # Get t-distribution quantile
            df_t = fit.params.get('nu', 5)
            from scipy import stats as sp_stats
            q_t = sp_stats.t.ppf(0.95, df_t)

            # GARCH intervals
            test_returns = returns_clean.iloc[cal_end:].values
            test_signal = signal_clean.iloc[cal_end:].values
            pred_mean = np.mean(returns_clean.iloc[:cal_end])

            lower_garch = pred_mean - q_t * cond_vol[:len(test_returns)]
            upper_garch = pred_mean + q_t * cond_vol[:len(test_returns)]

            high_vol = test_signal > np.median(test_signal)

            covered = (test_returns >= lower_garch) & (test_returns <= upper_garch)

            results.append({
                'factor': factor,
                'method': 'GARCH',
                'coverage_overall': np.mean(covered),
                'coverage_high_vol': np.mean(covered[high_vol]) if high_vol.sum() > 0 else np.nan,
                'coverage_low_vol': np.mean(covered[~high_vol]) if (~high_vol).sum() > 0 else np.nan,
                'avg_width': np.mean(2 * q_t * cond_vol[:len(test_returns)])
            })

        except Exception as e:
            print(f"  GARCH failed: {e}")

    return pd.DataFrame(results)


def generate_summary_tables(main_df: pd.DataFrame,
                            signal_df: pd.DataFrame,
                            subperiod_df: pd.DataFrame,
                            output_dir: Path):
    """Generate publication-ready summary tables."""

    # Table 1: Main Coverage Results
    print("\n" + "="*80)
    print("TABLE 1: Coverage Analysis by Method and Factor")
    print("="*80)

    pivot = main_df.pivot_table(
        index='factor',
        columns='method',
        values=['coverage_high_vol', 'improvement_high', 'p_value'],
        aggfunc='first'
    )

    print("\nHigh-Volatility Coverage (90% Target):")
    print("-" * 70)
    print(f"{'Factor':<10} {'Standard CP':>15} {'SA-CI':>15} {'Vol Scaling':>15}")
    print("-" * 70)

    factors = main_df['factor'].unique()
    for factor in factors:
        std = main_df[(main_df['factor'] == factor) & (main_df['method'] == 'standard')]
        saci = main_df[(main_df['factor'] == factor) & (main_df['method'] == 'saci')]
        vs = main_df[(main_df['factor'] == factor) & (main_df['method'] == 'vol_scaling')]

        if len(std) > 0 and len(saci) > 0:
            std_cov = std['coverage_high_vol'].values[0]
            std_se = std['se_high_vol'].values[0]
            saci_cov = saci['coverage_high_vol'].values[0]
            saci_se = saci['se_high_vol'].values[0]
            vs_cov = vs['coverage_high_vol'].values[0] if len(vs) > 0 else np.nan
            vs_se = vs['se_high_vol'].values[0] if len(vs) > 0 else np.nan

            print(f"{factor:<10} {std_cov:>6.1%} ({std_se:.1%}) "
                  f"{saci_cov:>6.1%} ({saci_se:.1%}) "
                  f"{vs_cov:>6.1%} ({vs_se:.1%})")

    print("-" * 70)

    # Averages
    std_avg = main_df[main_df['method'] == 'standard']['coverage_high_vol'].mean()
    saci_avg = main_df[main_df['method'] == 'saci']['coverage_high_vol'].mean()
    vs_avg = main_df[main_df['method'] == 'vol_scaling']['coverage_high_vol'].mean()

    print(f"{'Average':<10} {std_avg:>14.1%} {saci_avg:>15.1%} {vs_avg:>15.1%}")
    print(f"{'Improvement':10} {'---':>14} {saci_avg - std_avg:>+14.1%} {vs_avg - std_avg:>+14.1%}")

    # Statistical significance summary
    print("\n\nStatistical Significance (SA-CI vs Standard):")
    print("-" * 50)
    saci_results = main_df[main_df['method'] == 'saci']
    sig_count = (saci_results['p_value'] < 0.05).sum()
    total = len(saci_results)
    print(f"Factors with significant improvement (p<0.05): {sig_count}/{total}")

    # Save tables
    main_df.to_csv(output_dir / 'main_coverage_results.csv', index=False)
    signal_df.to_csv(output_dir / 'signal_validity.csv', index=False)
    subperiod_df.to_csv(output_dir / 'subperiod_analysis.csv', index=False)

    print(f"\nResults saved to {output_dir}")


def main():
    """Run complete analysis."""
    print("="*80)
    print("SIGNAL-ADAPTIVE CONFORMAL INFERENCE: COMPLETE ANALYSIS")
    print("="*80)
    print("\nThis analysis uses HONEST volatility signals, not 'crowding' proxies.")
    print("Algorithm has been FIXED to properly weight calibration scores.")

    # Load data
    factors = load_factor_data()
    if factors is None:
        print("ERROR: Could not load factor data")
        return

    print(f"\nData period: {factors.index.min()} to {factors.index.max()}")
    print(f"Observations: {len(factors)}")

    # ALL factors including Mkt-RF
    factor_names = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']
    available_factors = [f for f in factor_names if f in factors.columns]
    print(f"Analyzing factors: {available_factors}")

    output_dir = Path(__file__).parent.parent / 'results'
    output_dir.mkdir(exist_ok=True)

    # 1. Signal validity analysis (transparency)
    signal_df = run_signal_validity_analysis(factors, available_factors)

    # 2. Main coverage analysis
    main_df = run_main_coverage_analysis(factors, available_factors)

    # 3. Subperiod analysis (regime change)
    subperiod_df = run_subperiod_analysis(factors, available_factors)

    # 4. Monte Carlo (if time permits)
    print("\n\nRunning Monte Carlo (this may take a minute)...")
    mc_df = run_monte_carlo_validation(n_sim=100)
    mc_df.to_csv(output_dir / 'monte_carlo_results.csv', index=False)

    # 5. GARCH comparison
    garch_df = run_garch_comparison(factors, available_factors)
    if len(garch_df) > 0:
        garch_df.to_csv(output_dir / 'garch_comparison.csv', index=False)

    # Generate summary tables
    generate_summary_tables(main_df, signal_df, subperiod_df, output_dir)

    # Final summary
    print("\n" + "="*80)
    print("ANALYSIS COMPLETE")
    print("="*80)

    print("\nKey Findings:")
    print("-" * 60)

    std_high = main_df[main_df['method'] == 'standard']['coverage_high_vol'].mean()
    saci_high = main_df[main_df['method'] == 'saci']['coverage_high_vol'].mean()
    vs_high = main_df[main_df['method'] == 'vol_scaling']['coverage_high_vol'].mean()

    print(f"1. Standard CP under-covers during high-volatility periods:")
    print(f"   Average high-vol coverage: {std_high:.1%} (target: 90%)")

    print(f"\n2. Signal-Adaptive CI (SA-CI) improves coverage:")
    print(f"   Average high-vol coverage: {saci_high:.1%}")
    print(f"   Improvement: {saci_high - std_high:+.1%}")

    print(f"\n3. Vanilla volatility scaling performs similarly:")
    print(f"   Average high-vol coverage: {vs_high:.1%}")
    print(f"   Improvement: {vs_high - std_high:+.1%}")

    print(f"\n4. The signal is essentially realized volatility:")
    if len(signal_df) > 0:
        avg_corr = signal_df['vol_vs_abs_ret'].mean()
        print(f"   Correlation with 'crowding' proxy: {avg_corr:.2f}")

    print("\n" + "="*80)


if __name__ == '__main__':
    main()
